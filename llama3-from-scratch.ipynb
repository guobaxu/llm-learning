{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDK download model\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('wdndev/Meta-Llama-3-8B-Instruct-2layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer\n",
    "使用字节对编码（Byte Pair Encoding, BPE）作为分词器\n",
    "\n",
    "andrej karpathy 实现BPE tokenizer 的链接：\n",
    "\n",
    "创建分词器实例`tiktoken.Encoding`，其中主要设置文本拆分规则`pat_str`，合并规则`mergeable_ranks`，特殊的token`special_tokens`。\n",
    "\n",
    "正则表达式，\n",
    "1. `(?i:'s|'t|'re|'ve|'m|'ll|'d)`中`?i:`表示忽略大小写，其中这些缩写单独作为一个token；\n",
    "2. `[^\\r\\n\\p{L}\\p{N}]?\\p{L}+`：匹配既不是换行符、字母，也不是数字的字符（例如标点符号），后面跟着一个或多个字母；\n",
    "3. `\\p{N}{1,3}`匹配1到3位数字；\n",
    "4. `?[^\\s\\p{L}\\p{N}]+[\\r\\n]*`一个可选的空格，后面跟着一个或多个标点符号，再后面跟着 0 个或多个换行符；\n",
    "5. `\\s*[\\r\\n]+`匹配0 个或多个空白字符，后面跟着 1 个或多个换行符；\n",
    "6. `\\s+(?!\\S)`匹配1 个或多个空白字符，且这些空白字符后面不能有非空白字符，例如文本末尾的空格；\n",
    "7. `\\s+`匹配 1 个或多个空白字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15339, 1917, 0], 'hello world!')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的分词器\n",
    "tokenizer_path = \"wdndev/Meta-Llama-3-8B-Instruct-2layers/tokenizer.model\"\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "# 设置特殊token\n",
    "special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "# 创建token编码器\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    ")\n",
    "# 测试编码解码功能\n",
    "tokenizer.encode(\"hello world!\"),tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取模型文件\n",
    "\n",
    "因原版 Llama3 8B 模型32层 Transformers，如果加载全部的参数，16G内存机器加载失败，故选取原版 Llama3 8B 模型权重的前2层，重新保存，大小约为2.7G。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86984\\AppData\\Local\\Temp\\ipykernel_23140\\2382376065.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"wdndev\\Meta-Llama-3-8B-Instruct-2layers\\consolidated_2layers.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"tok_embeddings.weight\",\n",
      "    \"layers.0.attention.wq.weight\",\n",
      "    \"layers.0.attention.wk.weight\",\n",
      "    \"layers.0.attention.wv.weight\",\n",
      "    \"layers.0.attention.wo.weight\",\n",
      "    \"layers.0.feed_forward.w1.weight\",\n",
      "    \"layers.0.feed_forward.w3.weight\",\n",
      "    \"layers.0.feed_forward.w2.weight\",\n",
      "    \"layers.0.attention_norm.weight\",\n",
      "    \"layers.0.ffn_norm.weight\",\n",
      "    \"layers.1.attention.wq.weight\",\n",
      "    \"layers.1.attention.wk.weight\",\n",
      "    \"layers.1.attention.wv.weight\",\n",
      "    \"layers.1.attention.wo.weight\",\n",
      "    \"layers.1.feed_forward.w1.weight\",\n",
      "    \"layers.1.feed_forward.w3.weight\",\n",
      "    \"layers.1.feed_forward.w2.weight\",\n",
      "    \"layers.1.attention_norm.weight\",\n",
      "    \"layers.1.ffn_norm.weight\",\n",
      "    \"norm.weight\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 加载模型权重\n",
    "# 这里使用的是减少层数的参数文件，仅用于学习模型处理过程\n",
    "model = torch.load(\"wdndev\\Meta-Llama-3-8B-Instruct-2layers\\consolidated_2layers.pth\")\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim': 4096,\n",
       " 'n_layers': 2,\n",
       " 'n_heads': 32,\n",
       " 'n_kv_heads': 8,\n",
       " 'vocab_size': 128256,\n",
       " 'multiple_of': 1024,\n",
       " 'ffn_dim_multiplier': 1.3,\n",
       " 'norm_eps': 1e-05,\n",
       " 'rope_theta': 500000.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取模型配置参数\n",
    "with open(\"wdndev\\Meta-Llama-3-8B-Instruct-2layers\\params.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从配置文件中提取模型参数\n",
    "dim = config[\"dim\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "n_heads = config[\"n_heads\"]\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文本转换为token\n",
    "这里使用 tiktoken（OpenAI 的库）作为分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]\n",
      "['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "\n",
    "# 编码为token\n",
    "# <|begin_of_text|>的token_id=128000\n",
    "tokens = [128000] + tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "tokens = torch.tensor(tokens)\n",
    "\n",
    "# 将每个 token 解码为对应的文本\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]\n",
    "print(prompt_split_as_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将token转换为embedding\n",
    "这里使用内置的神经网络模块\n",
    "\n",
    "无论如何, [17x1] token 现在是 [17x4096]，即每个 token 的长度为 4096 的 embeddings\n",
    "\n",
    "注意：跟踪 shapes，这样一切将变得理解更容易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载嵌入层并复制权重\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
    "embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "\n",
    "# 获取未归一化的 token 嵌入\n",
    "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)\n",
    "token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建RMS 归一化嵌入\n",
    "请注意，经过此步骤后 shapes 不变， 只是值被归一化\n",
    "\n",
    "需要注意的是，需要一个 norm_eps（来自配置）以避免不小心将 RMS 设置为 0 并导致除以 0 的情况\n",
    "\n",
    "公式如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rms 归一化函数\n",
    "\n",
    "# def rms_norm(tensor, norm_weights):\n",
    "#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5\n",
    "#     return tensor * (norm_weights / rms)\n",
    "\n",
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建第一个Transformer层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.归一化\n",
    "从模型字典中访问 layer.0 （这是第一层）\n",
    "\n",
    "embedding input -> RMS_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 4096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 归一化token嵌入\n",
    "token_embeddings = rms_norm(token_embeddings_unnormalized, model[\"layers.0.attention_norm.weight\"])\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.从头实现注意力机制\n",
    "加载第一个 Transformer 层的注意力头\n",
    "\n",
    "当我们从模型中加载 query， key，value 和 output 向量时，注意到 shapes 分别为 [4096x4096]， [1024x4096]， [1024x4096]， [4096x4096]\n",
    "\n",
    "乍一看这有些奇怪，因为在理想情况下我们希望每个头单独拥有各自的 q，k，v 和 o\n",
    "\n",
    "这里作者将其捆绑在一起，为什么会这样呢? 因为这样有助于并行化注意力头的计算\n",
    "\n",
    "将展开所有内容..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "# 打印第一个层的注意力权重 shapes\n",
    "print(\n",
    "    model[\"layers.0.attention.wq.weight\"].shape,\n",
    "    model[\"layers.0.attention.wk.weight\"].shape,\n",
    "    model[\"layers.0.attention.wv.weight\"].shape,\n",
    "    model[\"layers.0.attention.wo.weight\"].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展开query\n",
    "在下一节中，将展开多个注意力头的 query，得到的 shapes 为 [32x128x4096]\n",
    "\n",
    "这里的 32 是 Llama3 的注意力头数量，128 是 query 向量的大小，4096 是 token 嵌入的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 4096])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape query 权重为[头数，头维度，嵌入维度]\n",
    "\n",
    "q_layer0 = model[\"layers.0.attention.wq.weight\"]\n",
    "head_dim = q_layer0.shape[0] // n_heads\n",
    "q_layer0 = q_layer0.view(n_heads, head_dim, dim)\n",
    "q_layer0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现第一层的第一个头\n",
    "这里查询了第一个层的第一个头的 query 权重矩阵，其大小为 [128x4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4096])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_layer0_head0 = q_layer0[0]\n",
    "q_layer0_head0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在将 query 权重与 token 嵌入相乘，以获得每个 token 的 query\n",
    "这里可以看到得到的 shape 是 [17x128]， 这是因为有 17 个 token，每个 token 有一个长度为 128 的 query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)\n",
    "q_per_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 位置编码\n",
    "当前，每个 token 都有一个 query 向量，但如果你想一想 -- 其实各个 query 向量并不知道它们在 prompt 中的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用复数点积计算旋转向量\n",
    "zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
    "zero_to_one_split_into_64_parts\n",
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "freqs_for_each_token = torch.outer(torch.arange(17), freqs)\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_per_token_split_into_pairs.shape torch.Size([17, 64, 2])\n",
      "q_per_token_as_complex_numbers.shape torch.Size([17, 64])\n",
      "q_per_token_as_complex_numbers_rotated.shape torch.Size([17, 64])\n",
      "q_per_token_split_into_pairs_rotated.shape torch.Size([17, 64, 2])\n",
      "q_per_token_rotated.shape torch.Size([17, 128])\n"
     ]
    }
   ],
   "source": [
    "q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "print(\"q_per_token_split_into_pairs.shape\",q_per_token_split_into_pairs.shape)\n",
    "\n",
    "q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "print(\"q_per_token_as_complex_numbers.shape\",q_per_token_as_complex_numbers.shape)\n",
    "\n",
    "q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis\n",
    "print(\"q_per_token_as_complex_numbers_rotated.shape\", q_per_token_as_complex_numbers_rotated.shape)\n",
    "\n",
    "q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)\n",
    "print(\"q_per_token_split_into_pairs_rotated.shape\", q_per_token_split_into_pairs_rotated.shape)\n",
    "\n",
    "q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "print(\"q_per_token_rotated.shape\", q_per_token_rotated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_layer0.shape torch.Size([8, 128, 4096])\n",
      "k_layer0_head0.shape torch.Size([128, 4096])\n",
      "k_per_token.shape torch.Size([17, 128])\n",
      "k_per_token_split_into_pairs.shape torch.Size([17, 64, 2])\n",
      "k_per_token_as_complex_numbers.shape torch.Size([17, 64])\n",
      "k_per_token_split_into_pairs_rotated.shape torch.Size([17, 64, 2])\n",
      "k_per_token_rotated.shape torch.Size([17, 128])\n"
     ]
    }
   ],
   "source": [
    "# key\n",
    "k_layer0 = model[\"layers.0.attention.wk.weight\"]\n",
    "k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)\n",
    "print(\"k_layer0.shape\", k_layer0.shape)\n",
    "\n",
    "k_layer0_head0 = k_layer0[0]\n",
    "print(\"k_layer0_head0.shape\", k_layer0_head0.shape)\n",
    "\n",
    "k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)\n",
    "print(\"k_per_token.shape\", k_per_token.shape)\n",
    "\n",
    "k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "print(\"k_per_token_split_into_pairs.shape\",k_per_token_split_into_pairs.shape)\n",
    "\n",
    "k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "print(\"k_per_token_as_complex_numbers.shape\", k_per_token_as_complex_numbers.shape)\n",
    "\n",
    "k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "print(\"k_per_token_split_into_pairs_rotated.shape\", k_per_token_split_into_pairs_rotated.shape)\n",
    "\n",
    "k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "print(\"k_per_token_rotated.shape\", k_per_token_rotated.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来，将 query 和 key 的矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5\n",
    "qk_per_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 屏蔽QK分数\n",
    "在 llama3 的训练过程中，未来的 token qk 分数被屏蔽。\n",
    "\n",
    "为什么？因为在训练过程中，只学习使用过去的 token 来预测 token 。\n",
    "\n",
    "因此，在推理过程中，将未来的 token 设置为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "def display_qk_heatmap(qk_per_token):\n",
    "    _, ax = plt.subplots()\n",
    "    im = ax.imshow(qk_per_token.to(float).detach(), cmap='viridis')\n",
    "    ax.set_xticks(range(len(prompt_split_as_tokens)))\n",
    "    ax.set_yticks(range(len(prompt_split_as_tokens)))\n",
    "    ax.set_xticklabels(prompt_split_as_tokens)\n",
    "    ax.set_yticklabels(prompt_split_as_tokens)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "# display_qk_heatmap(qk_per_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
