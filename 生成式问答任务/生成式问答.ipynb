{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成式问答模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集中每一行为一个数据样本，json 格式。\n",
    "其中，\"context\" 代表参考文章，\"question\"代表问题，\"answer\" 代表问题答案，\"id\"表示数据的index，从0开始\n",
    "\n",
    "{\"context\": \"违规分为:一般违规扣分、严重违规扣分、出售假冒商品违规扣分,淘宝网每年12月31日24:00点会对符合条件的扣分做清零处理,详情如下:|温馨提醒:由于出售假冒商品24≤N<48分,当年的24分不清零,所以会存在第一年和第二年的不同计分情况。\", \"answer\": \"12月31日24:00\", \"question\": \"淘宝扣分什么时候清零\", \"id\": 203}\n",
    "\n",
    "json文件中，同一个context和question可能有不同的answer，不同的answer属于不同的数据样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载函数，继承自Dataset类\n",
    "class DuReaderQG(Dataset):\n",
    "    def __init__(self, datafile):\n",
    "        self.data = self.load_data(datafile)\n",
    "\n",
    "    def load_data(self, datafile):\n",
    "        data = []\n",
    "        with open(datafile, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                json_data = json.loads(line)\n",
    "                data.append(json_data)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DuReaderQG(\"./DuReaderQG/train.json\")\n",
    "valid_data = DuReaderQG(\"./DuReaderQG/dev.json\")\n",
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.加载分词器和模型\n",
    "\n",
    "使用DataLoader加载数据，设计批处理函数，将文本转换为模型可以接受的 token IDs，并且构建对应的目标。\n",
    "\n",
    "加载mengzi-t5-base的分词器，使用T5Tokenizer加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用transformers的T5Tokenizer加载分词器\n",
    "# 这个分词器需要sentencepiece库，没有的话需要pip安装下\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./model/mengzi-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./model/mengzi-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型文件中有一个config.json文件，查看模型的配置：\n",
    "```\n",
    "{\n",
    "  \"architectures\": [\n",
    "    \"T5ForConditionalGeneration\"\n",
    "  ],\n",
    "  \"d_ff\": 2048,\n",
    "  \"d_kv\": 64,\n",
    "  \"d_model\": 768,\n",
    "  \"decoder_start_token_id\": 0,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"eos_token_id\": 1,\n",
    "  \"feed_forward_proj\": \"gated-gelu\",\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"initializer_factor\": 1.0,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"layer_norm_epsilon\": 1e-06,\n",
    "  \"model_type\": \"t5\",\n",
    "  \"num_decoder_layers\": 12,\n",
    "  \"num_heads\": 12,\n",
    "  \"num_layers\": 12,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"relative_attention_num_buckets\": 32,\n",
    "  \"tie_word_embeddings\": false,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.9.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 32128\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**测试一下分词器和模型的生成功能**\n",
    "\n",
    "但发现有点问题，如果input_text中有<extra_id_0>（比如）\"中国的首都位于<extra_id_0>。\"，就会正常生成答案。但是如果输入文本中没有<extra_id_0>这一token，结果就是空的。\n",
    "\n",
    "这样看的话<extra_id_0>有点类似于占位符的感觉，或者提示模型有这个token才是句子结尾，如果提问中没有这个token，按照概率自然就会输出这个token（ids是32127），解码出来就是<extra_id_0>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入文本\n",
    "input_text = \"中国的首都位于<extra_id_0>。\"\n",
    "# 对输入文本进行编码\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "print(f\"输入: {input_text}\")\n",
    "print(\"input_ids: \", input_ids)\n",
    "# 生成文本\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,  # 生成文本的最大长度\n",
    "    num_beams=5,    # Beam Search 的 beam 数量\n",
    "    no_repeat_ngram_size=2,  # 避免重复的 n-gram\n",
    "    early_stopping=True,     # 提前停止生成\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# 打印结果\n",
    "print(f\"生成: {generated_text}\")\n",
    "print(\"输出的ids: \",outputs)\n",
    "\n",
    "# 不跳过特殊token\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "# 打印结果\n",
    "print(\"不跳过特殊token\")\n",
    "print(f\"生成: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果输入文本中没有<extra_id_0>这一token，输出ids就只有32127，解码是<extra_id_0>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入文本\n",
    "input_text = \"中国的首都位于。\"\n",
    "# 对输入文本进行编码\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "print(f\"输入: {input_text}\")\n",
    "print(\"input_ids: \", input_ids)\n",
    "# 生成文本\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,  # 生成文本的最大长度\n",
    "    num_beams=5,    # Beam Search 的 beam 数量\n",
    "    no_repeat_ngram_size=2,  # 避免重复的 n-gram\n",
    "    early_stopping=True,     # 提前停止生成\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "# 打印结果\n",
    "print(f\"生成: {generated_text}\")\n",
    "print(\"输出的ids: \",outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.数据预处理\n",
    "\n",
    "生成式问答任务中，输入是context和question，标记是answer，这些都是文本，考虑将context和question拼接在一起作为输入，但可能会面临着文本长度超限，所以需要有个文本处理。\n",
    "\n",
    "对于超长文本的处理，要么直接截取文本要么就是分块处理，先考虑简单的截取这一措施，整个流程跑通之后再试试分块。\n",
    "\n",
    "**上下文分快处理（chunk）**\n",
    "输入文本拼接，仿照抽取式问答任务，将问题和上下文编码为下面的形式：question </s> context </s>。\n",
    "由于问题与上下文拼接后的 token 序列可能超过模型的最大输入长度，因此我们可以将上下文切分为短文本块 (chunk) 来处理，同时为了避免答案被截断，我们使用滑窗使得切分出的文本块之间有重叠。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单的拼接**\n",
    "\n",
    "将上下文拼接后，简单的限制最大文本长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置最大输入长度和最大答案文本长度\n",
    "max_input_length = 512\n",
    "max_target_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./model/mengzi-t5-base\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**批数据处理**\n",
    "\n",
    "使用分词器提供的`as_target_tokenizer()`函数来并行地对输入和标签进行分词，\n",
    "\n",
    "并且将标签序列中填充的 pad 字符设置为 -100 以便在计算交叉熵损失时忽略它们，\n",
    "\n",
    "通过模型自带的`prepare_decoder_input_ids_from_labels`函数对标签进行移位操作以准备好 decoder input IDs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义批处理函数\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_question, batch_targets = [], [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['context'])\n",
    "        batch_question.append(sample['question'])\n",
    "        batch_targets.append(sample['answer'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        batch_question,\n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出一个batch数据查看编码结果：`dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print('input_ids:',batch['input_ids'])\n",
    "print('decoder_input_ids:',batch['decoder_input_ids'])\n",
    "print('labels:',batch['labels'])\n",
    "# 解码这些label看看是不是正确的\n",
    "import numpy as np\n",
    "label_tokens = batch[\"labels\"].numpy()\n",
    "label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "# generated_text = tokenizer.decode(label, skip_special_tokens=True)\n",
    "decoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文使用 T5ForConditionalGeneration 构建模型，使用 T5ForConditionalGeneration 构造的模型已经封装好了对应的损失函数，并且计算出的损失会直接包含在模型的输出 outputs 中，可以直接通过 outputs.loss 获得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数，接口：dataloader，model，optimizer，lr_scheduler，\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        # 内置的loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**验证和测试**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试和验证函数中，通过 model.generate() 函数生成回答，然后计算生成回答和正确回答之间的差异，评价指标使用BLEU-1，BLEU-2，BLEU-3，BLEU-4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 BLEU 指标\n",
    "bleu_metric = load(\"bleu\")\n",
    "\n",
    "# 示例数据\n",
    "predictions = [\"The cat is on the mat\"]  # 模型生成的文本\n",
    "references = [\"The cat is not on the mat\"]  # 参考文本（可以有多个参考）\n",
    "# 计算 BLEU 指标\n",
    "results = bleu_metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "TOKENIZE_CHINESE = lambda x: ' '.join(x)\n",
    "# 示例数据\n",
    "predictions = [TOKENIZE_CHINESE(\"今天天气还不错\")]  # 模型生成的文本\n",
    "references = [TOKENIZE_CHINESE(\"今天天气很好\")]  # 参考文本（可以有多个参考）\n",
    "# 计算 BLEU 指标\n",
    "results = bleu_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    # preds,labels:list[]\n",
    "    b1, b2, b3, b4 = [], [], [], []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred = [pred]\n",
    "        references = [label]\n",
    "        # 计算 BLEU 指标\n",
    "        results = bleu_metric.compute(predictions=pred, references=references)\n",
    "        b1.append(results[\"precisions\"][0])\n",
    "        b2.append(results[\"precisions\"][1])\n",
    "        b3.append(results[\"precisions\"][2])\n",
    "        b4.append(results[\"precisions\"][3])\n",
    "    return {\n",
    "        \"bleu-1\": sum(b1) / len(b1),\n",
    "        \"bleu-2\": sum(b2) / len(b2),\n",
    "        \"bleu-3\": sum(b3) / len(b3),\n",
    "        \"bleu-4\": sum(b4) / len(b4),}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [TOKENIZE_CHINESE(\"今天天气还不错\"), \"The cat is on the mat\"]\n",
    "references = [TOKENIZE_CHINESE(\"今天天气很好\"), \"The cat is not on the mat\"]\n",
    "results = compute_metrics(predictions,references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model):\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels += [' '.join(label.strip()) for label in decoded_labels]\n",
    "\n",
    "    # 计算bleu指标,返回bleu1-4四个指标\n",
    "    result = compute_metrics(preds, labels)\n",
    "    print(f\"Bleu-1: {result['Bleu-1']:>0.2f} Blue-2: {result['Bleu-2']:>0.2f} Bleu-3: {result['Bleu-3']:>0.2f} Bleu-4: {result['Bleu-4']:>0.2f}\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "learning_rate = 2e-5\n",
    "epoch_num = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    bleu_score = test_loop(valid_dataloader, model)\n",
    "    print(bleu_score)\n",
    "    bleu = bleu_score['Bleu-4']\n",
    "    if bleu > best_bleu:\n",
    "        best_bleu = bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_bleu-4_{bleu:0.4f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"\n",
    "受众在哪里，媒体就应该在哪里，媒体的体制、内容、技术就应该向哪里转变。\n",
    "媒体融合关键是以人为本，即满足大众的信息需求，为受众提供更优质的服务。\n",
    "这就要求媒体在融合发展的过程中，既注重技术创新，又注重用户体验。\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    article_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "generated_tokens = model.generate(\n",
    "    input_ids[\"input_ids\"],\n",
    "    attention_mask=input_ids[\"attention_mask\"],\n",
    "    max_length=32,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")\n",
    "summary = tokenizer.decode(\n",
    "    generated_tokens[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = model.generate(\n",
    "    batch[\"input_ids\"],\n",
    "    attention_mask=batch[\"attention_mask\"],\n",
    "    max_length=max_target_length\n",
    ")\n",
    "if isinstance(generated_tokens, tuple):\n",
    "    generated_tokens = generated_tokens[0]\n",
    "label_tokens = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "preds = [' '.join(pred.strip()) for pred in decoded_preds]\n",
    "labels = [' '.join(label.strip()) for label in decoded_labels]\n",
    "print(preds)\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
